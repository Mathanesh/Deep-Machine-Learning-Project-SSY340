{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da1fa563-204b-4802-8a52-b0c6b453a8c8",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7dfdb5b7-f09d-454d-92ec-723110ff14cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import cv2\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1170992f-6c5d-40b7-a741-d3502b7e662d",
   "metadata": {},
   "source": [
    "### Dataset Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23254b88-418e-4273-bdcd-a8e98a277cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = \"/SCM_CHALMERS/GitHub/Deep-Machine-Learning-Project-SSY340/Datasets/comma10k/imgs\"\n",
    "mask_dir = \"/SCM_CHALMERS/GitHub/Deep-Machine-Learning-Project-SSY340/Datasets/comma10k/masks\"\n",
    "val_imgs_dir = \"/SCM_CHALMERS/GitHub/Deep-Machine-Learning-Project-SSY340/Datasets/comma10k/val_imgs\"\n",
    "val_masks_dir = \"/SCM_CHALMERS/GitHub/Deep-Machine-Learning-Project-SSY340/Datasets/comma10k/val_masks\"\n",
    "video_directory = \"/SCM_CHALMERS/GitHub/Deep-Machine-Learning-Project-SSY340/Videos/comma10k.mp4\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1827946-2a6c-43ba-af5f-3f56625318e0",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "281418cc-115b-4a7e-a916-73fd66124ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegDataset(Dataset):\n",
    "    def __init__(self, img_dir, mask_dir, transform=None):\n",
    "        self.transform = transform\n",
    "        self.img = sorted(glob.glob(os.path.join(img_dir, \"*.png\")))\n",
    "        self.mask = sorted(glob.glob(os.path.join(mask_dir, \"*.png\")))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        if torch.is_tensor(item):\n",
    "            item = item.tolist()\n",
    "\n",
    "        \"\"\"\n",
    "        Default Sizes: [3x1164x874]\n",
    "        \"\"\"\n",
    "        img = cv2.imread(self.img[item])\n",
    "        mask = cv2.imread(self.mask[item],0)\n",
    "\n",
    "        if self.transform:\n",
    "            norm = torchvision.transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "            img = norm(self.transform(img))\n",
    "            mask = self.transform(mask).squeeze(0)\n",
    "            mask[mask < 0.255] = 4.0\n",
    "            mask[mask < 0.35] = 3.0\n",
    "            mask[mask < 0.443] = 2.0\n",
    "            mask[mask < 0.54] = 1.0\n",
    "            mask[mask < 0.7] = 0.0\n",
    "\n",
    "        return img, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40331559-e7a4-47b7-91a7-59c0d5e956ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51b5c390-9322-4bd2-8078-0bdb8c5e2a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# network.py\n",
    "class SegNetV2(nn.Module):\n",
    "    def __init__(self, chkpt_dir=\"models\"):\n",
    "        super(SegNetV2,self).__init__()\n",
    "        self.file = os.path.join(chkpt_dir, \"segnet_v2\")\n",
    "        self.base_model = torchvision.models.segmentation.deeplabv3_resnet101(False, num_classes=5)\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.base_model(x)['out']\n",
    "\n",
    "    def save(self):\n",
    "        torch.save(self.state_dict(), self.file)\n",
    "\n",
    "    def load(self):\n",
    "        self.load_state_dict(torch.load(self.file))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c31a1368-4fc0-4b33-8401-21c4139422cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py\n",
    "def train(model, device, train_loader, val_loader, loss_fn, optimizer, epochs, load_model=False):\n",
    "    \"\"\"\n",
    "    :param model: Model for predictions\n",
    "    :param device: GPU or CPU\n",
    "    :param train_loader: Training set\n",
    "    :param val_loader: Validation Set\n",
    "    :param loss_fn: loss function\n",
    "    :param optimizer: Optimizer\n",
    "    :param epochs: amount of epochs\n",
    "    :param load_model: Loads a pretrained model\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    global total_loss\n",
    "    if load_model:\n",
    "        model.load()\n",
    "        print(\"Model Loaded\")\n",
    "\n",
    "    model.to(device)\n",
    "    best_score = np.inf\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    print(\"Starting...\")\n",
    "    for epoch in range(epochs):\n",
    "        loop = tqdm(train_loader, desc=f\"Training Epoch #{epoch}\")\n",
    "        val_loop = tqdm(val_loader, desc=f\"Validation Epoch #{epoch}\")\n",
    "        total_loss = 0\n",
    "        for i, (img, mask) in enumerate(loop):\n",
    "            img = img.to(device, dtype=torch.float32)  # NxCxHxW\n",
    "            mask = mask.to(device, dtype=torch.long)  # NxHxW\n",
    "\n",
    "            for p in model.parameters():\n",
    "                p.grad = None\n",
    "\n",
    "            # Forward Pass\n",
    "            with torch.cuda.amp.autocast():\n",
    "                pred = model(img)  # NxClassxHxW\n",
    "                loss = loss_fn(pred, mask)\n",
    "\n",
    "            # Backward Pass\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "\n",
    "        with torch.no_grad():\n",
    "            \"\"\"\n",
    "            Checks model performance on the validation set\n",
    "            \"\"\"\n",
    "            val_loss = 0\n",
    "            for j, (input, target) in enumerate(val_loop):\n",
    "                input = input.to(device, dtype=torch.float32)\n",
    "                target = target.to(device, dtype=torch.long)\n",
    "\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    prediction = model(input)\n",
    "                    v_loss = loss_fn(prediction, target)\n",
    "\n",
    "                val_loss += v_loss.item()\n",
    "                val_loop.set_postfix(loss=v_loss.item())\n",
    "\n",
    "        # # Save model if it does well on the validation set\n",
    "        if val_loss < best_score:\n",
    "            best_score = val_loss\n",
    "            model.save()\n",
    "            print(\"Model saved.\")\n",
    "\n",
    "        print(f'Training Loss {total_loss:.3f} \\t Validation Loss {val_loss:.3f}')\n",
    "\n",
    "\n",
    "def visualize(directory, model, transform, device):\n",
    "    \"\"\"\n",
    "    Takes video and turn them into images.\n",
    "    Saves the images into a folder.\n",
    "    :param directory:Where the images saved\n",
    "    :param model:Model for prediction\n",
    "    :param transform:Convert images to tensor\n",
    "    :param device: GPU or CPU\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    model.load()\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        video = cv2.VideoCapture(directory)\n",
    "        success, image = video.read()\n",
    "        count = 0\n",
    "        while success:\n",
    "            img = transform(image)\n",
    "            img = img.to(device, dtype=torch.float32).unsqueeze(0)\n",
    "            pred = model(img).argmax(1)\n",
    "            imshow(img, pred, count)\n",
    "            success, image = video.read()\n",
    "            count += 1\n",
    "\n",
    "\n",
    "\n",
    "def imshow(img, mask, count=0):\n",
    "    \"\"\"\n",
    "    Saves image to a folder\n",
    "    :param img: Image\n",
    "    :param mask: Mask\n",
    "    :param count: Frame count\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    invTrans = transforms.Compose([transforms.Normalize(mean=[0., 0., 0.],\n",
    "                                                        std=[1 / 0.229, 1 / 0.224, 1 / 0.225]),\n",
    "                                   transforms.Normalize(mean=[-0.485, -0.456, -0.406],\n",
    "                                                        std=[1., 1., 1.]),\n",
    "                                   ])\n",
    "    resize = transforms.Resize((360,640))\n",
    "    save_dir = \"D:\\\\VIDEOS\\\\segnet_V2\\\\img\" + str(count) + \".png\"\n",
    "    img, mask = img.cpu(), mask.detach().cpu()\n",
    "    img, mask = resize(img), resize(mask)\n",
    "    img = invTrans(img)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img[0].permute(1, 2, 0), cmap='gray')\n",
    "    plt.imshow(mask[0], cmap='coolwarm', alpha=0.4)\n",
    "    plt.savefig(save_dir, bbox_inches='tight', pad_inches=0)\n",
    "    # plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea21789-9d61-41b9-bd17-452f71a5c57d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\condawin\\envs\\dml\\Lib\\site-packages\\torchvision\\models\\_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n",
      "  warnings.warn(\n",
      "D:\\condawin\\envs\\dml\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet101-63fe2227.pth\" to C:\\Users\\Mathanesh/.cache\\torch\\hub\\checkpoints\\resnet101-63fe2227.pth\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 171M/171M [00:01<00:00, 111MB/s]\n",
      "C:\\Users\\Mathanesh\\AppData\\Local\\Temp\\ipykernel_17932\\1702331884.py:21: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "D:\\condawin\\envs\\dml\\Lib\\site-packages\\torch\\amp\\grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch #0:   0%|                                                                      | 0/4444 [00:00<?, ?it/s]\n",
      "Validation Epoch #0:   0%|                                                                     | 0/500 [00:00<?, ?it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "# Needed for training\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "BATCH_SIZE = 2\n",
    "EPOCHS = 20\n",
    "IMAGE_SIZE = 256\n",
    "NUM_WORKERS = 4\n",
    "PIN_MEM = True\n",
    "SEED = 89\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Preprocess dataset\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "])\n",
    "\n",
    "# Video Prepreocess\n",
    "video_preprocess =transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# Load the datasets needed.\n",
    "# Shape: 3 x 256 x 256 image\n",
    "trainset = SegDataset(img_dir, mask_dir, transform=preprocess)\n",
    "train_loader = DataLoader(trainset, BATCH_SIZE, num_workers=NUM_WORKERS, pin_memory=PIN_MEM, shuffle=True)\n",
    "\n",
    "valset = SegDataset(val_imgs_dir, val_masks_dir, transform=preprocess)\n",
    "val_loader = DataLoader(valset, BATCH_SIZE, num_workers=NUM_WORKERS, pin_memory=PIN_MEM)\n",
    "\n",
    "# Load Model\n",
    "model = SegNetV2()\n",
    "# Loss Function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "# Train the model\n",
    "train(model, device, train_loader, val_loader, loss_fn, optimizer, EPOCHS)\n",
    "\n",
    "# Visualize\n",
    "visualize(video_directory, model, video_preprocess, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87965e93-ace4-49d1-9f56-1620dc4090a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
